[["index.html", "Other Case Study Chapter 1 Introduction", " Other Case Study Regina Galambos 2021-07-14 Chapter 1 Introduction This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["theory.html", "Chapter 2 Theory", " Chapter 2 Theory You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 3. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. "],["bayes-rule.html", "2.1 Bayes’ Rule", " 2.1 Bayes’ Rule \\[\\begin{equation*} p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)} \\end{equation*}\\] knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "],["dirichlet-distribution.html", "2.2 Dirichlet Distribution", " 2.2 Dirichlet Distribution Beta and stuff "],["multinomial-distribution.html", "2.3 Multinomial Distribution", " 2.3 Multinomial Distribution Binomial and stuff "],["mixture-models.html", "2.4 Mixture Models", " 2.4 Mixture Models what’s that "],["lda-theory.html", "Chapter 3 Latent Dirichlet Allocation", " Chapter 3 Latent Dirichlet Allocation We describe our methods in this chapter. Math can be added in body using usual syntax like this "],["what-is-lda.html", "3.1 What is LDA?", " 3.1 What is LDA? \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this1. We will approximate standard error to 0.0272 where we mention \\(p = \\frac{a}{b}\\)↩︎ \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\]↩︎ "],["how-does-it-work.html", "3.2 How does it work?", " 3.2 How does it work? difficult math things go here "],["associatedpress-dataset.html", "3.3 AssociatedPress Dataset", " 3.3 AssociatedPress Dataset library(tidyverse) library(tidytext) library(tm) library(topicmodels) library(topicdoc) library(stringr) library(wordcloud) library(RColorBrewer) data(&quot;AssociatedPress&quot;, package = &quot;topicmodels&quot;) 3.3.1 Document-Term-Matrix AssociatedPress &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; Non-/sparse entries: 302031/23220327 Sparsity : 99% Maximal term length: 18 Weighting : term frequency (tf) The data set is an object of class “DocumentTermMatrix” provided by package tm. It is a document-term matrix which contains the term frequency of 10473 terms in 2246 documents. terms &lt;- Terms(AssociatedPress) summary(terms) Length Class Mode 10473 character character head(terms) [1] &quot;aaron&quot; &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; &quot;abbott&quot; &quot;abboud&quot; ap_td &lt;- tidy(AssociatedPress) ap_td # A tibble: 302,031 x 3 document term count &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 adding 1 2 1 adult 2 3 1 ago 1 4 1 alcohol 1 5 1 allegedly 1 6 1 allen 1 7 1 apparently 2 8 1 appeared 1 9 1 arrested 1 10 1 assault 1 # … with 302,021 more rows 3.3.2 Split data for training &amp; test # Select first 500 articles full_data &lt;- AssociatedPress[1:500, ] # create train and test sets n &lt;- nrow(full_data) splitter &lt;- sample(1:n, round(n * 0.8)) train_set &lt;- full_data[splitter, ] test_set &lt;- full_data[-splitter, ] 3.3.3 Train models for different k # fit models with different k n_topics &lt;- c(2, 4, 10, 20, 50, 100) ap_lda_models &lt;- n_topics %&gt;% map(LDA, x = train_set, control = list(seed = 42)) # Select model with k = 2 ap_lda_models[[1]] A LDA_VEM topic model with 2 topics. 3.3.4 Word-topic-probabilities for k = 2 # Word-Topic-probabilities ap_topics &lt;- tidy(ap_lda_models[[1]], matrix = &quot;beta&quot;) ap_topics # A tibble: 20,946 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 5.27e- 5 2 2 aaron 2.60e- 5 3 1 abandon 5.27e- 5 4 2 abandon 8.60e-11 5 1 abandoned 3.62e- 5 6 2 abandoned 1.20e- 4 7 1 abandoning 2.47e-78 8 2 abandoning 2.60e- 5 9 1 abbott 3.27e- 8 10 2 abbott 7.79e- 5 # … with 20,936 more rows 3.3.5 Top Words in Topics ap_top_terms &lt;- ap_topics %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + scale_y_reordered() ### Document-topic probabilities ap_documents &lt;- tidy(ap_lda_models[[1]], matrix = &quot;gamma&quot;) ap_documents # A tibble: 800 x 3 document topic gamma &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 1 0.496 2 2 1 0.999 3 3 1 0.000449 4 4 1 0.990 5 5 1 1.00 6 6 1 0.000996 7 7 1 0.654 8 8 1 0.000289 9 9 1 0.998 10 10 1 1.00 # … with 790 more rows 3.3.6 Top 5 words in each topic ap_lda_td &lt;- tidy(ap_lda_models[[1]]) top_terms &lt;- ap_lda_td %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms # A tibble: 10 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 percent 0.00783 2 1 soviet 0.00713 3 1 new 0.00574 4 1 government 0.00542 5 1 last 0.00430 6 2 i 0.00791 7 2 people 0.00513 8 2 new 0.00476 9 2 two 0.00378 10 2 years 0.00373 3.3.7 Word-topic-probabilities for k = 20 # Word-Topic-probabilities ap_topics_20 &lt;- tidy(ap_lda_models[[4]], matrix = &quot;beta&quot;) ap_topics_20 # A tibble: 209,460 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 3.72e-44 2 2 aaron 0 3 3 aaron 3.72e-44 4 4 aaron 1.07e-15 5 5 aaron 3.72e-44 6 6 aaron 0 7 7 aaron 3.72e-44 8 8 aaron 0 9 9 aaron 3.72e-44 10 10 aaron 3.72e-44 # … with 209,450 more rows 3.3.8 Top Words in Topics ap_top_terms &lt;- ap_topics_20 %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol = 3) + coord_flip() 3.3.9 How well does the model predict? - Evaluate with Perplexity data_frame(k = n_topics, perplex = map_dbl(ap_lda_models, perplexity)) %&gt;% ggplot(aes(k, perplex)) + geom_point() + geom_line() + labs(title = &quot;Evaluating LDA topic models on training set&quot;, subtitle = &quot;Optimal number of topics (smaller is better)&quot;, x = &quot;Number of topics&quot;, y = &quot;Perplexity&quot;) 3.3.9.1 k = 2 print(&quot;Test set - Model 1&quot;) [1] &quot;Test set - Model 1&quot; perplexity(ap_lda_models[[1]], newdata = test_set) [1] 9654.721 3.3.9.2 k = 4 print(&quot;Test set - Model 2&quot;) [1] &quot;Test set - Model 2&quot; perplexity(ap_lda_models[[2]], newdata = test_set) [1] 8210.824 3.3.9.3 k = 10 print(&quot;Training set - Model 3&quot;) [1] &quot;Training set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = train_set) [1] 5240.172 print(&quot;Test set - Model 3&quot;) [1] &quot;Test set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = test_set) [1] 6070.499 3.3.9.4 k = 20 print(&quot;Test set - Model 4&quot;) [1] &quot;Test set - Model 4&quot; perplexity(ap_lda_models[[4]], newdata = test_set) [1] 4668.823 3.3.9.5 k = 50 print(&quot;Test set - Model 5&quot;) [1] &quot;Test set - Model 5&quot; perplexity(ap_lda_models[[5]], newdata = test_set) [1] 2980.262 3.3.9.6 k = 100 print(&quot;Test set - Model 6&quot;) [1] &quot;Test set - Model 6&quot; perplexity(ap_lda_models[[6]], newdata = test_set) [1] 1924.436 "],["case-studies.html", "Chapter 4 Case Studies ", " Chapter 4 Case Studies "],["associatedpress-dataset-1.html", "4.1 AssociatedPress Dataset", " 4.1 AssociatedPress Dataset library(tidyverse) library(tidytext) library(tm) library(topicmodels) library(topicdoc) library(stringr) library(wordcloud) library(RColorBrewer) data(&quot;AssociatedPress&quot;, package = &quot;topicmodels&quot;) 4.1.1 Document-Term-Matrix AssociatedPress &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; Non-/sparse entries: 302031/23220327 Sparsity : 99% Maximal term length: 18 Weighting : term frequency (tf) The data set is an object of class “DocumentTermMatrix” provided by package tm. It is a document-term matrix which contains the term frequency of 10473 terms in 2246 documents. terms &lt;- Terms(AssociatedPress) summary(terms) Length Class Mode 10473 character character head(terms) [1] &quot;aaron&quot; &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; &quot;abbott&quot; &quot;abboud&quot; ap_td &lt;- tidy(AssociatedPress) ap_td # A tibble: 302,031 x 3 document term count &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 adding 1 2 1 adult 2 3 1 ago 1 4 1 alcohol 1 5 1 allegedly 1 6 1 allen 1 7 1 apparently 2 8 1 appeared 1 9 1 arrested 1 10 1 assault 1 # … with 302,021 more rows 4.1.2 Split data for training &amp; test # Select first 500 articles full_data &lt;- AssociatedPress[1:500, ] # create train and test sets n &lt;- nrow(full_data) splitter &lt;- sample(1:n, round(n * 0.8)) train_set &lt;- full_data[splitter, ] test_set &lt;- full_data[-splitter, ] 4.1.3 Train models for different k # fit models with different k n_topics &lt;- c(2, 4, 10, 20, 50, 100) ap_lda_models &lt;- n_topics %&gt;% map(LDA, x = train_set, control = list(seed = 42)) # Select model with k = 2 ap_lda_models[[1]] A LDA_VEM topic model with 2 topics. 4.1.4 Word-topic-probabilities for k = 2 # Word-Topic-probabilities ap_topics &lt;- tidy(ap_lda_models[[1]], matrix = &quot;beta&quot;) ap_topics # A tibble: 20,946 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 5.19e- 5 2 2 aaron 2.65e- 5 3 1 abandon 2.60e- 5 4 2 abandon 5.94e-14 5 1 abandoned 8.33e- 5 6 2 abandoned 2.07e- 4 7 1 abandoning 3.72e-44 8 2 abandoning 3.72e-44 9 1 abbott 1.92e-17 10 2 abbott 7.96e- 5 # … with 20,936 more rows 4.1.5 Top Words in Topics ap_top_terms &lt;- ap_topics %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + scale_y_reordered() ### Document-topic probabilities ap_documents &lt;- tidy(ap_lda_models[[1]], matrix = &quot;gamma&quot;) ap_documents # A tibble: 800 x 3 document topic gamma &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 1 0.727 2 2 1 0.000647 3 3 1 0.000480 4 4 1 0.00182 5 5 1 1.00 6 6 1 1.00 7 7 1 0.000699 8 8 1 0.00102 9 9 1 0.453 10 10 1 0.999 # … with 790 more rows 4.1.6 Top 5 words in each topic ap_lda_td &lt;- tidy(ap_lda_models[[1]]) top_terms &lt;- ap_lda_td %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms # A tibble: 10 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 percent 0.00646 2 1 soviet 0.00608 3 1 new 0.00527 4 1 government 0.00503 5 1 year 0.00453 6 2 i 0.00825 7 2 people 0.00469 8 2 new 0.00435 9 2 state 0.00431 10 2 president 0.00387 4.1.7 Word-topic-probabilities for k = 20 # Word-Topic-probabilities ap_topics_20 &lt;- tidy(ap_lda_models[[4]], matrix = &quot;beta&quot;) ap_topics_20 # A tibble: 209,460 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 5.77e-192 2 2 aaron 1.83e-121 3 3 aaron 9.53e-192 4 4 aaron 6.00e-157 5 5 aaron 3.07e-191 6 6 aaron 3.73e-157 7 7 aaron 1.22e-191 8 8 aaron 1.49e-191 9 9 aaron 1.17e-191 10 10 aaron 8.71e-192 # … with 209,450 more rows 4.1.8 Top Words in Topics ap_top_terms &lt;- ap_topics_20 %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol = 3) + coord_flip() 4.1.9 How well does the model predict? - Evaluate with Perplexity data_frame(k = n_topics, perplex = map_dbl(ap_lda_models, perplexity)) %&gt;% ggplot(aes(k, perplex)) + geom_point() + geom_line() + labs(title = &quot;Evaluating LDA topic models on training set&quot;, subtitle = &quot;Optimal number of topics (smaller is better)&quot;, x = &quot;Number of topics&quot;, y = &quot;Perplexity&quot;) 4.1.9.1 k = 2 print(&quot;Test set - Model 1&quot;) [1] &quot;Test set - Model 1&quot; perplexity(ap_lda_models[[1]], newdata = test_set) [1] 370639.4 4.1.9.2 k = 4 print(&quot;Test set - Model 2&quot;) [1] &quot;Test set - Model 2&quot; perplexity(ap_lda_models[[2]], newdata = test_set) [1] 362083.7 4.1.9.3 k = 10 print(&quot;Training set - Model 3&quot;) [1] &quot;Training set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = train_set) [1] 1792.711 print(&quot;Test set - Model 3&quot;) [1] &quot;Test set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = test_set) [1] 377601.8 4.1.9.4 k = 20 print(&quot;Test set - Model 4&quot;) [1] &quot;Test set - Model 4&quot; perplexity(ap_lda_models[[4]], newdata = test_set) [1] 399509.3 4.1.9.5 k = 50 print(&quot;Test set - Model 5&quot;) [1] &quot;Test set - Model 5&quot; perplexity(ap_lda_models[[5]], newdata = test_set) [1] 401079.5 4.1.9.6 k = 100 print(&quot;Test set - Model 6&quot;) [1] &quot;Test set - Model 6&quot; perplexity(ap_lda_models[[6]], newdata = test_set) [1] 416945 "],["r-markdown.html", "4.2 R Markdown", " 4.2 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["r-markdown-1.html", "4.3 R Markdown", " 4.3 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "]]

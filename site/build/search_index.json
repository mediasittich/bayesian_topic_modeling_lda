[["index.html", "Bayesian Topic Modelling: Latent Dirichlet Allocation and its Applications Chapter 1 Introduction", " Bayesian Topic Modelling: Latent Dirichlet Allocation and its Applications Regina Galambos 2021-07-07 Chapter 1 Introduction This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["theory.html", "Chapter 2 Theory", " Chapter 2 Theory You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 3. Figures and tables with captions will be placed in figure and table environments, respectively. Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. "],["bayes-rule.html", "2.1 Bayes’ Rule", " 2.1 Bayes’ Rule \\[\\begin{equation*} p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)} \\end{equation*}\\] Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "],["dirichlet-distribution.html", "2.2 Dirichlet Distribution", " 2.2 Dirichlet Distribution Beta and stuff "],["multinomial-distribution.html", "2.3 Multinomial Distribution", " 2.3 Multinomial Distribution Binomial and stuff "],["mixture-models.html", "2.4 Mixture Models", " 2.4 Mixture Models what’s that "],["lda-theory.html", "Chapter 3 Latent Dirichlet Allocation", " Chapter 3 Latent Dirichlet Allocation We describe our methods in this chapter. Math can be added in body using usual syntax like this "],["what-is-lda.html", "3.1 What is LDA?", " 3.1 What is LDA? \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this1. We will approximate standard error to 0.0272 where we mention \\(p = \\frac{a}{b}\\)↩︎ \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\]↩︎ "],["how-does-it-work.html", "3.2 How does it work?", " 3.2 How does it work? difficult math things go here "],["case-study.html", "Chapter 4 Case Study", " Chapter 4 Case Study Some significant applications are demonstrated in this chapter. 4.0.1 Document-Term-Matrix &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; Non-/sparse entries: 302031/23220327 Sparsity : 99% Maximal term length: 18 Weighting : term frequency (tf) The data set is an object of class “DocumentTermMatrix” provided by package tm. It is a document-term matrix which contains the term frequency of 10473 terms in 2246 documents. Length Class Mode 10473 character character [1] &quot;aaron&quot; &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; &quot;abbott&quot; [6] &quot;abboud&quot; # A tibble: 302,031 x 3 document term count &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 adding 1 2 1 adult 2 3 1 ago 1 4 1 alcohol 1 5 1 allegedly 1 6 1 allen 1 7 1 apparently 2 8 1 appeared 1 9 1 arrested 1 10 1 assault 1 # … with 302,021 more rows A LDA_VEM topic model with 2 topics. 4.0.2 Word-topic probabilities # A tibble: 20,946 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 7.50e- 6 2 2 aaron 3.37e- 5 3 1 abandon 2.91e- 5 4 2 abandon 3.81e- 5 5 1 abandoned 8.42e- 5 6 2 abandoned 9.71e- 5 7 1 abandoning 7.03e- 6 8 2 abandoning 1.85e- 5 9 1 abbott 2.85e-18 10 2 abbott 3.11e- 5 # … with 20,936 more rows Kind of fuzzy… 4.0.3 Document-topic probabilities # A tibble: 4,492 x 3 document topic gamma &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 1 0.360 2 2 1 0.714 3 3 1 0.275 4 4 1 0.488 5 5 1 0.00264 6 6 1 0.779 7 7 1 0.603 8 8 1 0.135 9 9 1 0.976 10 10 1 0.759 # … with 4,482 more rows document topic gamma Min. : 1.0 Min. :1 Min. :0.0003151 1st Qu.: 562.2 1st Qu.:1 1st Qu.:0.0028051 Median :1123.5 Median :1 Median :0.2945853 Mean :1123.5 Mean :1 Mean :0.4160884 3rd Qu.:1684.8 3rd Qu.:1 3rd Qu.:0.8347125 Max. :2246.0 Max. :1 Max. :0.9997184 # A tibble: 287 x 3 document term count &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 6 noriega 16 2 6 panama 12 3 6 jackson 6 4 6 powell 6 5 6 administration 5 6 6 economic 5 7 6 general 5 8 6 i 5 9 6 panamanian 5 10 6 american 4 # … with 277 more rows A LDA_VEM topic model with 12 topics. # A tibble: 60 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 court 0.00852 2 1 trial 0.00807 3 1 case 0.00769 4 1 attorney 0.00708 5 1 judge 0.00704 6 2 house 0.0107 7 2 bill 0.00906 8 2 congress 0.00904 9 2 i 0.00857 10 2 senate 0.00800 # … with 50 more rows "]]

[["index.html", "AssoitedPress Dataset Chapter 1 Introduction", " AssoitedPress Dataset Regina Galambos 2021-07-13 Chapter 1 Introduction This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["theory.html", "Chapter 2 Theory", " Chapter 2 Theory You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 3. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. "],["bayes-rule.html", "2.1 Bayes’ Rule", " 2.1 Bayes’ Rule \\[\\begin{equation*} p(\\theta|x) = \\frac{p(x|\\theta)p(\\theta)}{p(x)} \\end{equation*}\\] knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2021) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). "],["dirichlet-distribution.html", "2.2 Dirichlet Distribution", " 2.2 Dirichlet Distribution Beta and stuff "],["multinomial-distribution.html", "2.3 Multinomial Distribution", " 2.3 Multinomial Distribution Binomial and stuff "],["mixture-models.html", "2.4 Mixture Models", " 2.4 Mixture Models what’s that "],["lda-theory.html", "Chapter 3 Latent Dirichlet Allocation", " Chapter 3 Latent Dirichlet Allocation We describe our methods in this chapter. Math can be added in body using usual syntax like this "],["what-is-lda.html", "3.1 What is LDA?", " 3.1 What is LDA? \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\] You can also use math in footnotes like this1. We will approximate standard error to 0.0272 where we mention \\(p = \\frac{a}{b}\\)↩︎ \\(p\\) is unknown but expected to be around 1/3. Standard error will be approximated \\[ SE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027 \\]↩︎ "],["how-does-it-work.html", "3.2 How does it work?", " 3.2 How does it work? difficult math things go here "],["associatedpress-dataset.html", "3.3 AssociatedPress Dataset", " 3.3 AssociatedPress Dataset library(tidyverse) library(tidytext) library(tm) library(topicmodels) library(topicdoc) library(stringr) library(wordcloud) library(RColorBrewer) data(&quot;AssociatedPress&quot;, package = &quot;topicmodels&quot;) 3.3.1 Document-Term-Matrix AssociatedPress &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; Non-/sparse entries: 302031/23220327 Sparsity : 99% Maximal term length: 18 Weighting : term frequency (tf) The data set is an object of class “DocumentTermMatrix” provided by package tm. It is a document-term matrix which contains the term frequency of 10473 terms in 2246 documents. terms &lt;- Terms(AssociatedPress) summary(terms) Length Class Mode 10473 character character head(terms) [1] &quot;aaron&quot; &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; [5] &quot;abbott&quot; &quot;abboud&quot; ap_td &lt;- tidy(AssociatedPress) ap_td # A tibble: 302,031 x 3 document term count &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 adding 1 2 1 adult 2 3 1 ago 1 4 1 alcohol 1 5 1 allegedly 1 6 1 allen 1 7 1 apparently 2 8 1 appeared 1 9 1 arrested 1 10 1 assault 1 # … with 302,021 more rows 3.3.2 Split data for training &amp; test # Select first 500 articles full_data &lt;- AssociatedPress[1:500, ] # create train and test sets n &lt;- nrow(full_data) splitter &lt;- sample(1:n, round(n * 0.8)) train_set &lt;- full_data[splitter, ] test_set &lt;- full_data[-splitter, ] 3.3.3 Train models for different k # fit models with different k n_topics &lt;- c(2, 4, 10, 20, 50, 100) ap_lda_models &lt;- n_topics %&gt;% map(LDA, x = train_set, control = list(seed = 42)) # Select model with k = 2 ap_lda_models[[1]] A LDA_VEM topic model with 2 topics. 3.3.4 Word-topic-probabilities for k = 2 # Word-Topic-probabilities ap_topics &lt;- tidy(ap_lda_models[[1]], matrix = &quot;beta&quot;) ap_topics # A tibble: 20,946 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 5.30e- 5 2 2 aaron 2.60e- 5 3 1 abandon 5.30e- 5 4 2 abandon 4.19e- 8 5 1 abandoned 4.62e- 5 6 2 abandoned 2.15e- 4 7 1 abandoning 9.36e-84 8 2 abandoning 2.60e- 5 9 1 abbott 2.07e-14 10 2 abbott 7.81e- 5 # … with 20,936 more rows 3.3.5 Top Words in Topics ap_top_terms &lt;- ap_topics %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + scale_y_reordered() ### Document-topic probabilities ap_documents &lt;- tidy(ap_lda_models[[1]], matrix = &quot;gamma&quot;) ap_documents # A tibble: 800 x 3 document topic gamma &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 1 0.636 2 2 1 1.00 3 3 1 0.000543 4 4 1 0.484 5 5 1 0.758 6 6 1 0.000385 7 7 1 0.813 8 8 1 0.00131 9 9 1 0.321 10 10 1 0.238 # … with 790 more rows 3.3.6 Top 5 words in each topic ap_lda_td &lt;- tidy(ap_lda_models[[1]]) top_terms &lt;- ap_lda_td %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms # A tibble: 10 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 percent 0.00912 2 1 soviet 0.00623 3 1 new 0.00613 4 1 year 0.00511 5 1 government 0.00486 6 2 i 0.00806 7 2 people 0.00572 8 2 police 0.00457 9 2 state 0.00433 10 2 years 0.00364 3.3.7 Word-topic-probabilities for k = 20 # Word-Topic-probabilities ap_topics_20 &lt;- tidy(ap_lda_models[[4]], matrix = &quot;beta&quot;) ap_topics_20 # A tibble: 209,460 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 3.57e-191 2 2 aaron 2.77e- 4 3 3 aaron 1.24e-191 4 4 aaron 4.58e-157 5 5 aaron 1.25e-191 6 6 aaron 4.17e-191 7 7 aaron 6.97e-192 8 8 aaron 9.60e-192 9 9 aaron 6.89e-192 10 10 aaron 1.20e-191 # … with 209,450 more rows 3.3.8 Top Words in Topics ap_top_terms &lt;- ap_topics_20 %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol = 3) + coord_flip() 3.3.9 How well does the model predict? - Evaluate with Perplexity data_frame(k = n_topics, perplex = map_dbl(ap_lda_models, perplexity)) %&gt;% ggplot(aes(k, perplex)) + geom_point() + geom_line() + labs(title = &quot;Evaluating LDA topic models on training set&quot;, subtitle = &quot;Optimal number of topics (smaller is better)&quot;, x = &quot;Number of topics&quot;, y = &quot;Perplexity&quot;) 3.3.9.1 k = 2 print(&quot;Test set - Model 1&quot;) [1] &quot;Test set - Model 1&quot; perplexity(ap_lda_models[[1]], newdata = test_set) [1] 505688 3.3.9.2 k = 4 print(&quot;Test set - Model 2&quot;) [1] &quot;Test set - Model 2&quot; perplexity(ap_lda_models[[2]], newdata = test_set) [1] 501075 3.3.9.3 k = 10 print(&quot;Training set - Model 3&quot;) [1] &quot;Training set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = train_set) [1] 1766.576 print(&quot;Test set - Model 3&quot;) [1] &quot;Test set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = test_set) [1] 525357.9 3.3.9.4 k = 20 print(&quot;Test set - Model 4&quot;) [1] &quot;Test set - Model 4&quot; perplexity(ap_lda_models[[4]], newdata = test_set) [1] 547085.4 3.3.9.5 k = 50 print(&quot;Test set - Model 5&quot;) [1] &quot;Test set - Model 5&quot; perplexity(ap_lda_models[[5]], newdata = test_set) [1] 558738.6 3.3.9.6 k = 100 print(&quot;Test set - Model 6&quot;) [1] &quot;Test set - Model 6&quot; perplexity(ap_lda_models[[6]], newdata = test_set) [1] 584442.5 "],["case-studies.html", "Chapter 4 Case Studies ", " Chapter 4 Case Studies "],["associatedpress-dataset-1.html", "4.1 AssociatedPress Dataset", " 4.1 AssociatedPress Dataset library(tidyverse) library(tidytext) library(tm) library(topicmodels) library(topicdoc) library(stringr) library(wordcloud) library(RColorBrewer) data(&quot;AssociatedPress&quot;, package = &quot;topicmodels&quot;) 4.1.1 Document-Term-Matrix AssociatedPress &lt;&lt;DocumentTermMatrix (documents: 2246, terms: 10473)&gt;&gt; Non-/sparse entries: 302031/23220327 Sparsity : 99% Maximal term length: 18 Weighting : term frequency (tf) The data set is an object of class “DocumentTermMatrix” provided by package tm. It is a document-term matrix which contains the term frequency of 10473 terms in 2246 documents. terms &lt;- Terms(AssociatedPress) summary(terms) Length Class Mode 10473 character character head(terms) [1] &quot;aaron&quot; &quot;abandon&quot; &quot;abandoned&quot; &quot;abandoning&quot; [5] &quot;abbott&quot; &quot;abboud&quot; ap_td &lt;- tidy(AssociatedPress) ap_td # A tibble: 302,031 x 3 document term count &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 adding 1 2 1 adult 2 3 1 ago 1 4 1 alcohol 1 5 1 allegedly 1 6 1 allen 1 7 1 apparently 2 8 1 appeared 1 9 1 arrested 1 10 1 assault 1 # … with 302,021 more rows 4.1.2 Split data for training &amp; test # Select first 500 articles full_data &lt;- AssociatedPress[1:500, ] # create train and test sets n &lt;- nrow(full_data) splitter &lt;- sample(1:n, round(n * 0.8)) train_set &lt;- full_data[splitter, ] test_set &lt;- full_data[-splitter, ] 4.1.3 Train models for different k # fit models with different k n_topics &lt;- c(2, 4, 10, 20, 50, 100) ap_lda_models &lt;- n_topics %&gt;% map(LDA, x = train_set, control = list(seed = 42)) # Select model with k = 2 ap_lda_models[[1]] A LDA_VEM topic model with 2 topics. 4.1.4 Word-topic-probabilities for k = 2 # Word-Topic-probabilities ap_topics &lt;- tidy(ap_lda_models[[1]], matrix = &quot;beta&quot;) ap_topics # A tibble: 20,946 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 3.73e- 5 2 2 aaron 6.95e- 5 3 1 abandon 5.26e- 5 4 2 abandon 1.66e-11 5 1 abandoned 7.50e- 5 6 2 abandoned 2.19e- 4 7 1 abandoning 4.54e-97 8 2 abandoning 2.69e- 5 9 1 abbott 4.38e-95 10 2 abbott 5.38e- 5 # … with 20,936 more rows 4.1.5 Top Words in Topics ap_top_terms &lt;- ap_topics %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder_within(term, beta, topic)) %&gt;% ggplot(aes(beta, term, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;) + scale_y_reordered() ### Document-topic probabilities ap_documents &lt;- tidy(ap_lda_models[[1]], matrix = &quot;gamma&quot;) ap_documents # A tibble: 800 x 3 document topic gamma &lt;int&gt; &lt;int&gt; &lt;dbl&gt; 1 1 1 0.449 2 2 1 0.000623 3 3 1 0.136 4 4 1 1.00 5 5 1 0.000978 6 6 1 1.00 7 7 1 0.000449 8 8 1 0.999 9 9 1 0.000361 10 10 1 1.00 # … with 790 more rows 4.1.6 Top 5 words in each topic ap_lda_td &lt;- tidy(ap_lda_models[[1]]) top_terms &lt;- ap_lda_td %&gt;% group_by(topic) %&gt;% top_n(5, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) top_terms # A tibble: 10 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 percent 0.00705 2 1 soviet 0.00617 3 1 new 0.00511 4 1 government 0.00494 5 1 year 0.00437 6 2 i 0.00827 7 2 people 0.00501 8 2 new 0.00456 9 2 state 0.00404 10 2 president 0.00394 4.1.7 Word-topic-probabilities for k = 20 # Word-Topic-probabilities ap_topics_20 &lt;- tidy(ap_lda_models[[4]], matrix = &quot;beta&quot;) ap_topics_20 # A tibble: 209,460 x 3 topic term beta &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 aaron 5.31e-297 2 2 aaron 2.43e- 4 3 3 aaron 5.51e-300 4 4 aaron 3.81e-296 5 5 aaron 1.02e-298 6 6 aaron 4.36e-298 7 7 aaron 7.81e-299 8 8 aaron 6.41e-291 9 9 aaron 1.29e-300 10 10 aaron 1.07e-296 # … with 209,450 more rows 4.1.8 Top Words in Topics ap_top_terms &lt;- ap_topics_20 %&gt;% group_by(topic) %&gt;% slice_max(beta, n = 10) %&gt;% ungroup() %&gt;% arrange(topic, -beta) ap_top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_bar(stat = &quot;identity&quot;, show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol = 3) + coord_flip() 4.1.9 How well does the model predict? - Evaluate with Perplexity data_frame(k = n_topics, perplex = map_dbl(ap_lda_models, perplexity)) %&gt;% ggplot(aes(k, perplex)) + geom_point() + geom_line() + labs(title = &quot;Evaluating LDA topic models on training set&quot;, subtitle = &quot;Optimal number of topics (smaller is better)&quot;, x = &quot;Number of topics&quot;, y = &quot;Perplexity&quot;) 4.1.9.1 k = 2 print(&quot;Test set - Model 1&quot;) [1] &quot;Test set - Model 1&quot; perplexity(ap_lda_models[[1]], newdata = test_set) [1] 208620.8 4.1.9.2 k = 4 print(&quot;Test set - Model 2&quot;) [1] &quot;Test set - Model 2&quot; perplexity(ap_lda_models[[2]], newdata = test_set) [1] 202410.2 4.1.9.3 k = 10 print(&quot;Training set - Model 3&quot;) [1] &quot;Training set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = train_set) [1] 1828.297 print(&quot;Test set - Model 3&quot;) [1] &quot;Test set - Model 3&quot; perplexity(ap_lda_models[[3]], newdata = test_set) [1] 214433.4 4.1.9.4 k = 20 print(&quot;Test set - Model 4&quot;) [1] &quot;Test set - Model 4&quot; perplexity(ap_lda_models[[4]], newdata = test_set) [1] 216808.3 4.1.9.5 k = 50 print(&quot;Test set - Model 5&quot;) [1] &quot;Test set - Model 5&quot; perplexity(ap_lda_models[[5]], newdata = test_set) [1] 219663.5 4.1.9.6 k = 100 print(&quot;Test set - Model 6&quot;) [1] &quot;Test set - Model 6&quot; perplexity(ap_lda_models[[6]], newdata = test_set) [1] 227192.6 "]]
